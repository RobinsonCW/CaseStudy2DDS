---
title: "Attrition and Salary Case Study"
author: "Chance Robinson"
date: "12/5/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

We will explore the case study data to gain insights into employee attrition and salary trends from which will will make predictions from our trained models.  Estimating attrition would be considered a classification problem and we'll be comparing and contrasting model performance based on ROC curves and other accuracy based measures.  For the salary predictions, we will be using Linear Regression techniques and grading the model performance based on RMSE (Root Mean Squared Error) and Adjusted R-Squared.

# Exploratory Data Analysis

## Library Imports


```{r library-imports, quietely=TRUE, warn.conflicts=FALSE}
# General imports
library(tidyverse)
library(knitr)
library(kableExtra)

# Correlation Matrix
library(ggcorrplot)
library(Hmisc)

# KNN
library(class)

# Naive Bayes
library(naivebayes)

# Support Vector Machines
library(e1071)  

# Random Forest
library(randomForest)  

# downSample
library(caret)

# ROC Curves
library(ROCR)
library(pROC)

```

## Load the CSV Data

```{r load-data}
data <- read.csv("../data/CaseStudy2-data.csv", stringsAsFactors=TRUE, header = TRUE)

# data <- read.csv("../data/CaseStudy2-data.csv", stringsAsFactors=TRUE, header = TRUE)
# data <- read.csv("../data/CaseStudy2-data.csv", stringsAsFactors=TRUE, header = TRUE)

```

## Default Output

```{r head-data}
kable(head(data)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
  
```

### Identify Dimensions

There are 870 rows and 36 colums in the dataset.
 
```{r data-dimensions}
dim(data)
```

### Data Columns

Here are the column names

```{r data-columns}
colnames(data)
```

### Convert Integers to Strings


These numeric columns will be converted into strings 

```{r data-integers-to-strings}

data$ID <- as.character(data$ID)
data$EmployeeNumber <- as.character(data$EmployeeNumber)
data$EmployeeCount <- as.character(data$EmployeeCount)
data$StandardHours <- as.character(data$StandardHours)
data$Over18 <- as.character(data$Over18)

```


### Convert Integers to Factors

These integer based columns with be converted into factors.

```{r data-integers-to-factors}

data$JobInvolvement <- factor(data$JobInvolvement, ordered = TRUE, 
                              levels = c(1, 2, 3, 4),
                              labels = c("Low", "Medium", "High", "Very High"))

data$JobSatisfaction <- factor(data$JobSatisfaction, ordered = TRUE, 
                              levels = c(1, 2, 3, 4),
                              labels = c("Low", "Medium", "High", "Very High"))

data$PerformanceRating <- factor(data$PerformanceRating, ordered = TRUE, 
                              levels = c(1, 2, 3, 4),
                              labels = c("Low", "Good", "Excellent", "Outstanding"))

data$RelationshipSatisfaction <- factor(data$RelationshipSatisfaction, ordered = TRUE, 
                              levels = c(1, 2, 3, 4),
                              labels = c("Low", "Medium", "High", "Very High"))

data$WorkLifeBalance <- factor(data$WorkLifeBalance, ordered = TRUE, 
                              levels = c(1, 2, 3, 4),
                              labels = c("Bad", "Better", "Good", "Best"))

### THIS WAS NOT ACTUALLY PROVIDED ON THE WALL
data$EnvironmentSatisfaction <- factor(data$EnvironmentSatisfaction, ordered = TRUE, 
                              levels = c(1, 2, 3, 4),
                              labels = c("Low", "Medium", "High", "Very High"))

data$StockOptionLevel <- factor(data$StockOptionLevel, ordered = TRUE, 
                              levels = c(0, 1, 2, 3),
                              labels = c("Zero", "One", "Two", "Three"))

data$JobLevel <- factor(data$JobLevel, ordered = TRUE, 
                              levels = c(1, 2, 3, 4, 5),
                              labels = c("One", "Two", "Three", "Four", "Five"))

data$Education <- factor(data$Education, ordered = FALSE, 
                              levels = c(1, 2, 3, 4, 5),
                              labels = c("One", "Two", "Three", "Four", "Five"))


```


### Describe the Data Types

```{r data-describe}
str(data)
```


### Numeric Columns

#### Column Names

```{r data-numeric}
data.numeric <- data %>%
  select_if(is.numeric)
```

#### Summary Tables

```{r data-numeric-summary}
kable(summary(data.numeric)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```


### Plots

```{r data-non-numeric-bar-plot-YearsSinceLastPromotion}

# DistanceFromHome
# NumCompaniesWorked
# TotalWorkingYears
# TrainingTimesLastYear
# YearsAtCompany

ggplot(data=data.numeric, aes(data.numeric$YearsSinceLastPromotion)) + 
  geom_bar() +
  ggtitle("Years since last promotion") + 
  labs(x = "Years", y = "Count")


```


### Non-Numeric Columns

#### Column Names

```{r data-non-numeric}
data.non.numeric <- data %>%
  select_if(is.factor)

colnames(data.non.numeric)
```

#### Counts and Percentages

```{r data-non-numeric-frequency-table, message=FALSE, error=FALSE, warning=FALSE}

kable(gather(data.non.numeric, "Column", "Value") %>%
  count(Column, Value) %>%
  group_by(Column) %>%             
  mutate(Percentage = prop.table(n)) %>%
  rename(Count = n)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```


### Correlation matrix for quantitative data

```{r corr-matrix}
# function for flattening and ordering the correlation matrix
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    correlation  =(cormat)[ut],
    p.value = pmat[ut]
    )
}
# See what variables are correlated with each other, p-values
correlation.matrix <- rcorr(as.matrix(data.numeric))
corDF <- data.frame(flattenCorrMatrix(correlation.matrix$r, correlation.matrix$P))
# Order the correlation matrix to show the highest correlated
# data.frame(corDF[order(-corDF$cor),])

quantDataModel <- corDF[which(abs(corDF$correlation) >= 0.5),]

kable(data.frame(quantDataModel[order(-abs(quantDataModel$cor)),])) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

### Correlation Plot

```{r corr.plot, fig.height = 10, fig.width = 10, fig.align = "center"}

correlation.plot <- round(cor(data.numeric), 1)
ggcorrplot(correlation.plot, method = "square", type = "full", lab = TRUE)

```

# Objective I

Model Comparisons for Attrition

## KNN

### Prepare Dataframe

```{r data-preparation-knn}

cols_to_keep = c("YearsSinceLastPromotion", "YearsWithCurrManager", "TrainingTimesLastYear", "DistanceFromHome", "NumCompaniesWorked", "YearsInCurrentRole", "TotalWorkingYears", "JobInvolvement", "OverTime", "BusinessTravel", "Attrition")

cols_to_remove <- c("ID", "EmployeeNumber", "EmployeeCount", "StandardHours", "Over18")

data.mod <- data %>%
  select(cols_to_keep)

# head(data.mod)

```



### Train / Test Split

```{r knn-train-test-split}

set.seed(1234)

sample.data <- sample_frac(data.mod, 1)

split.perc = .70

train.indices = sample(1:dim(sample.data)[1],round(split.perc * dim(sample.data)[1]))

train = sample.data[train.indices,]
test = sample.data[-train.indices,]

train <- upSample(train, as.factor(train$Attrition), list = FALSE)
train$Class <- NULL

# library(DMwR)
# train <- SMOTE(Attrition ~ ., train, perc.over = 100, perc.under=200)


```


### Build the model

```{r knn-model}

set.seed(1234)
ctrl <- trainControl(method="cv")
knn.model <- train(Attrition ~ ., data = train, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 10)

knn.model


```

### Make predictions

```{r knn-predict}

knn.predict <- predict(knn.model, newdata = test)

```


### Test Performance

```{r knn-performance-test}

confusionMatrix(knn.predict, test$Attrition, "Yes")

```


### Area Under the Curve

```{r knn-auc-plot-test, fig.align = "center", message=FALSE, error=FALSE, warning=FALSE}

# ?pROC

auc <- roc(as.integer(test$Attrition), as.integer(knn.predict))


g <- ggroc(auc, alpha = 0.5, colour = "red", linetype = 2, size = 2) +
  theme_minimal() + 
  ggtitle(paste('AUC of Test Set:', round(auc$auc[[1]],4))) + 
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
  

plot(g)



```




## Naive Bayes

## Prepare Dataframe

```{r data-preparation-naivebayes}

cols_to_remove <- c("ID", "EmployeeNumber", "EmployeeCount", "StandardHours", "Over18")


data.mod <- data %>%
  select(-cols_to_remove) %>%
  mutate(Attrition = factor(Attrition, labels = c("No", "Yes"))) 

# summary(data.mod)


```

### Train / Test Split

```{r naivebayes-train-test-split}

set.seed(1234)

sample.data <- sample_frac(data.mod, 1)

split.perc = .70

train.indices = sample(1:dim(sample.data)[1],round(split.perc * dim(sample.data)[1]))

train = sample.data[train.indices,]
test = sample.data[-train.indices,]

train <- upSample(train, as.factor(train$Attrition), list = FALSE)
train$Class <- NULL

# library(DMwR)
# train <- SMOTE(Attrition ~ ., train, perc.over = 100, perc.under=200)


```

### Build the model

```{r naivebayes-model, message=FALSE, error=FALSE, warning=FALSE}

model.nb.train <- naive_bayes(Attrition ~ ., data = train, laplace = TRUE, usekernel = TRUE)


```

### Make predictions

```{r naivebayes-predict, message=FALSE, error=FALSE, warning=FALSE}

p1 <- predict(model.nb.train, train)
p2 <- predict(model.nb.train, test)

```


### Test Performance

```{r naivebayes-performance-test}


confusionMatrix(data=p2,  
                reference=test$Attrition, "Yes")

```


### Area Under the Curve

```{r naivebayes-auc-plot-test, fig.align = "center", message=FALSE, error=FALSE, warning=FALSE}

# ?pROC

auc <- roc(as.integer(test$Attrition), as.integer(p2))


g <- ggroc(auc, alpha = 0.5, colour = "red", linetype = 2, size = 2) +
  theme_minimal() + 
  ggtitle(paste('AUC of Test Set:', round(auc$auc[[1]],4))) + 
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
  

plot(g)



```




## Support Vector Machines

### Prepare Dataframe

```{r data-preparation-svm}

cols_to_remove <- c("ID", "EmployeeNumber", "EmployeeCount", "StandardHours", "Over18")


data.mod <- data %>%
  select(-cols_to_remove) %>%
  mutate(Attrition = factor(Attrition, labels = c("No", "Yes"))) 

# summary(data.mod)


```

### Train / Test Split

```{r svm-boost-train-test-split, message=FALSE, error=FALSE, warning=FALSE}

set.seed(1234)

sample.data <- sample_frac(data.mod, 1)

split.perc = .70

train.indices = sample(1:dim(sample.data)[1],round(split.perc * dim(sample.data)[1]))

train = sample.data[train.indices,]
test = sample.data[-train.indices,]

# train <- upSample(train, as.factor(train$Attrition), list = FALSE)
# train$Class <- NULL

library(DMwR)
train <- SMOTE(Attrition ~ ., train, perc.over = 100, perc.under=200)


```

### Build the model

```{r svm-boost-model}

sum <- sum(table(test$Attrition))
wts <- table(test$Attrition) / sum

wts

svm.model <- svm(Attrition ~ ., 
           data = train,
           cost = 4,
           class.weights= c("No" = 0.4, "Yes" = 1),
           scale=TRUE
           )


summary(svm.model)

     
```

### Make predictions

```{r svm-predict}

prd <- predict(svm.model, test)

```


### Test performance

```{r svm-performance-test}

confusionMatrix(data=prd,  
                reference=test$Attrition, "Yes")


```


### Area Under the Curve

```{r svm-auc-plot-test, fig.align = "center", message=FALSE, error=FALSE, warning=FALSE}

# ?pROC

auc <- roc(as.integer(test$Attrition), as.integer(prd))


g <- ggroc(auc, alpha = 0.5, colour = "red", linetype = 2, size = 2) +
  theme_minimal() + 
  ggtitle(paste('AUC of Test Set:', round(auc$auc[[1]],4))) + 
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
  

plot(g)



```






## Random Forrest

### Prepare Dataframe

```{r data-preparation}

data <- read.csv("../data/CaseStudy2-data.csv", stringsAsFactors=TRUE, header = TRUE)

data$ID <- as.character(data$ID)
data$EmployeeNumber <- as.character(data$EmployeeNumber)
data$EmployeeCount <- as.character(data$EmployeeCount)
data$StandardHours <- as.character(data$StandardHours)
data$Over18 <- as.character(data$Over18)


cols_to_remove <- c("ID", "EmployeeNumber", "EmployeeCount", "StandardHours", "Over18")


data.mod <- data %>%
  select(-cols_to_remove) %>%
  mutate(Attrition = factor(Attrition, labels = c("No", "Yes"))) 


# summary(data.mod)


```


### Train / Test Split

```{r rf-train-test-split}

set.seed(1234)

split.perc = .70

train.indices = sample(1:dim(data.mod)[1],round(split.perc * dim(data.mod)[1]))

train = data.mod[train.indices,]
test = data.mod[-train.indices,]

# UPSAMPLING
# train <- upSample(train, train$Attrition, list = FALSE)
# train$Class <- NULL

library(DMwR)
train <- SMOTE(Attrition ~ ., train, perc.over = 100, perc.under=200)

# head(train)


```


### Build the model

```{r rf-model}

model.rf.train <- randomForest(Attrition ~ ., 
                               data = train, 
                               ntree = 150, 
                               mtry = 10, 
                               cutoff = c(0.55,1-0.55)
                               )

print(model.rf.train)

# ?na.action


p1 <- predict(model.rf.train, train, type="response")
p2 <- predict(model.rf.train, test, type="response")


plot(model.rf.train) 

varImp(model.rf.train)

# head(train)

```


### Make predictions

```{r rf-predict}

prd <- predict(model.rf.train, test)

```


### Test performance

```{r rf-performance-test}


confusionMatrix(data=prd,  
                reference=test$Attrition, "Yes")

```

### Area Under the Curve

```{r rf-auc-plot-test, fig.align = "center", message=FALSE, error=FALSE, warning=FALSE}

# ?pROC

auc <- roc(as.integer(test$Attrition), as.integer(prd))


g <- ggroc(auc, alpha = 0.5, colour = "red", linetype = 2, size = 2) +
  theme_minimal() + 
  ggtitle(paste('AUC of Test Set:', round(auc$auc[[1]],4))) + 
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
  

plot(g)


```

### Tune Performance

```{r rf-tune}

# tuneRF(train[,-2], train[,2],
#        stepFactor = 0.5,
#        ntreeTry = 150,
#        trace = TRUE,
#        improve = 0.5
# )
       
```




